{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep Learnings has been introduced to NLP tasks in 2010s. One of the earliest approach is RNN and LSTM which work best for sequence like input. In this Notebook, we will experiment two different LSTM model on our IMDB dataset. Then, similar to Notebook 3, we will use pre-trained Word2Vec as input layer and see if it helps. Finally, we will use CNN which is a Deep Learning technique commonly used in Computer Vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from lib.dataset import download_tfds_imdb_as_tensor_subword_8k, download_tfds_imdb_as_text_tiny, download_tfds_imdb_as_text\n",
    "from lib.deep_learning import run_lstm_pipeline, run_cnn_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = download_tfds_imdb_as_text()\n",
    "dataset_tiny = download_tfds_imdb_as_text_tiny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM\n",
    "\n",
    "We will try two slightly different uses of LSTM in text classification\n",
    "\n",
    "- BiLSTMLastStateClassification - We use the Bi-directional LSTM to encode a vector that represent the meaning of the whole sentence, then feed it to two layers of fully connected neural networks to make a prediction. This model is very different from what we have done in Notebook 1 - 3 in a sense that it takes \"order\" in to account (all previous experiments are bag-of-word). For example, it can differentiate this classic example `This movie is boring it is not good` and `This movie is good it is not boring`. Although bigrams can differentiate this particular example, in the real world there are many other examples that bigram is not enough and you need 3-gram, 4-gram. Increasing n-gram is not the solution because it introduced sparseness to feature space. That's why LSTM comes in. The drawback of this model is that if the sentence is very long, the last output state may encode very little information of the very first tokens of the sentence. \n",
    "\n",
    "- BiLSTMPoolClassification - This is another use of LSTM. Instead of using the output of the last state of LSTM, now we use the LSTM output of every token in the sentence and, sum them up, and feed to two layers of FC layer. Since we sum up to output vector of each token, we go back to the bag-of-word approach again. However, it's not \"very\" bag-of-word. This approach is different from other bag-of-word style we used previously e.g. the Word2Vec bag-of-word in Notebook 2 - 3. In those models, we the word representing each tokens are static i.e. the vector for `good` are always same regarding where it presents in the sentence. However, in this architecture, the vector representing each token is contextualized It is aware of the word nearby because it's the output of LSTM. It can learn that vector for `good` can be different in different context. Moreover, since we use every LSTM output of each token, they also include the last state output as we used in above model. Although this vector is corresponding to the last token, it also represents the meaning of the whole sentence.\n",
    "\n",
    "The  BiLSTMLastStateClassification is very similar to the TensorFlow official tutorial [here](https://www.tensorflow.org/tutorials/text/text_generation). However, we use the words as features to streamline with other experiments from Notebook 1 -4 while the tutorial use subwords. The model used in that tutorial has the F1 of 0.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-af256bab063e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_lstm_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiLSTMLastStateClassification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/lstm.py\u001b[0m in \u001b[0;36mrun_lstm_pipeline\u001b[0;34m(dataset, model, embeddings_size, hidden_unit, dropout, word2vec)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_lstm_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mrun_lstm_pipeline_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/lstm.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mX_train_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mX_test_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/nlp_utils.py\u001b[0m in \u001b[0;36mspacy_tokenizer\u001b[0;34m(sents, lower, lemma, ignore, use_cache, do_preprocess, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdoc_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_or_create_spacy_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_preprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtok_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/procore-data/lib/python3.7/site-packages/spacy/tokens/_serialize.py\u001b[0m in \u001b[0;36mget_docs\u001b[0;34m(self, vocab)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morth\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0morth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morth_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_user_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_lstm_pipeline(dataset, \"BiLSTMLastStateClassification\", embeddings_size=64, hidden_unit=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-04d0b6134d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_lstm_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiLSTMPoolClassification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/lstm.py\u001b[0m in \u001b[0;36mrun_lstm_pipeline\u001b[0;34m(dataset, model, embeddings_size, hidden_unit, dropout, word2vec)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_lstm_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mrun_lstm_pipeline_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/lstm.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mX_train_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mX_test_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/patavee/sentiment-analysis-from-probabilistic-to-deep-learning/lib/nlp_utils.py\u001b[0m in \u001b[0;36mspacy_tokenizer\u001b[0;34m(sents, lower, lemma, ignore, use_cache, do_preprocess, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtok_obj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mignore\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_ignore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_lstm_pipeline(dataset, \"BiLSTMPoolClassification\", embeddings_size=64, hidden_unit=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM with Pre-trained Word2Vec on Tiny Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will see if it helps if we initialize the embeddings with pre-trained Word2Vec. Similar to the Notebook 3, the idea is to see if it's useful to introduce the knowledge of the language to the model via pre-trained Word2Vec. Since the dimension of Word2Vec is 300, we have to change our embeddings layer size to be 300.\n",
    "\n",
    "**Prerequisite**\n",
    "\n",
    "Download [Google Word2Vec Model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) to this directory and run \n",
    "\n",
    "```\n",
    "gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "```\n",
    "If you already have those files or you don't want to save it in this directory, you can either change constant variable PRETRAINED_WV_MODEL_PATH  and PRETRAINED_GLOVE_MODEL_PATH or create symbolic link.\n",
    "    \n",
    "```\n",
    "ln -s /path/to/your/word2vec ./GoogleNews-vectors-negative300.bin\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings from disk - take about 5 mins to run\n",
    "import gensim \n",
    "PRETRAINED_WV_MODEL_PATH = \"./GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PRETRAINED_WV_MODEL_PATH, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.69, Accuracy: 0.55, Test Loss: 0.66, Test Accuracy: 0.61, Time: 388.91 s\n",
      "Epoch 2, Loss: 0.56, Accuracy: 0.74, Test Loss: 0.49, Test Accuracy: 0.80, Time: 396.00 s\n",
      "Epoch 3, Loss: 0.53, Accuracy: 0.75, Test Loss: 0.46, Test Accuracy: 0.81, Time: 395.56 s\n",
      "Epoch 4, Loss: 0.38, Accuracy: 0.86, Test Loss: 0.45, Test Accuracy: 0.81, Time: 393.17 s\n",
      "Epoch 5, Loss: 0.34, Accuracy: 0.88, Test Loss: 0.42, Test Accuracy: 0.82, Time: 395.48 s\n",
      "Epoch 6, Loss: 0.29, Accuracy: 0.90, Test Loss: 0.40, Test Accuracy: 0.84, Time: 391.67 s\n"
     ]
    }
   ],
   "source": [
    "run_lstm_pipeline(dataset, \"BiLSTMLastStateClassification\", embeddings_size=300, hidden_unit=10, word2vec=word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lstm_pipeline(dataset, \"BiLSTMLastStateClassification\", embeddings_size=300, hidden_unit=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN\n",
    "\n",
    "CNN is widely used in Computer Vision. However, recent NLP researchers also adopt CV for many different NLP tasks. The CNN is different from LSTM in the sense that it examines few tokens within window size (5 in our experiment) then move to the next window. Then it pools (taking average in our experiment) the output from each windows to the single vector. Intuitively, the CNN should be able to capture the local semantic within the window size better.\n",
    "\n",
    "This [paper](https://arxiv.org/pdf/1702.01923.pdf) discuss the comparative study of CNN and RNN for NLP tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.69, Accuracy: 0.53, Test Loss: 0.69, Test Accuracy: 0.61, Time: 37.38 s\n",
      "Epoch 2, Loss: 0.67, Accuracy: 0.68, Test Loss: 0.66, Test Accuracy: 0.72, Time: 35.38 s\n",
      "Epoch 3, Loss: 0.62, Accuracy: 0.77, Test Loss: 0.60, Test Accuracy: 0.78, Time: 35.57 s\n",
      "Epoch 4, Loss: 0.54, Accuracy: 0.82, Test Loss: 0.53, Test Accuracy: 0.82, Time: 35.42 s\n",
      "Epoch 5, Loss: 0.47, Accuracy: 0.86, Test Loss: 0.47, Test Accuracy: 0.83, Time: 35.37 s\n",
      "Epoch 6, Loss: 0.41, Accuracy: 0.88, Test Loss: 0.42, Test Accuracy: 0.86, Time: 35.76 s\n",
      "Epoch 7, Loss: 0.36, Accuracy: 0.89, Test Loss: 0.39, Test Accuracy: 0.87, Time: 35.83 s\n",
      "Epoch 8, Loss: 0.32, Accuracy: 0.90, Test Loss: 0.36, Test Accuracy: 0.88, Time: 35.73 s\n",
      "Epoch 9, Loss: 0.29, Accuracy: 0.91, Test Loss: 0.34, Test Accuracy: 0.88, Time: 35.67 s\n",
      "Epoch 10, Loss: 0.26, Accuracy: 0.92, Test Loss: 0.33, Test Accuracy: 0.88, Time: 35.74 s\n",
      "Epoch 11, Loss: 0.24, Accuracy: 0.93, Test Loss: 0.31, Test Accuracy: 0.89, Time: 35.71 s\n",
      "Epoch 12, Loss: 0.23, Accuracy: 0.93, Test Loss: 0.31, Test Accuracy: 0.89, Time: 35.93 s\n",
      "Epoch 13, Loss: 0.21, Accuracy: 0.94, Test Loss: 0.30, Test Accuracy: 0.89, Time: 35.27 s\n",
      "Epoch 14, Loss: 0.19, Accuracy: 0.94, Test Loss: 0.29, Test Accuracy: 0.89, Time: 35.89 s\n",
      "Epoch 15, Loss: 0.18, Accuracy: 0.95, Test Loss: 0.29, Test Accuracy: 0.89, Time: 35.69 s\n",
      "Epoch 16, Loss: 0.17, Accuracy: 0.95, Test Loss: 0.29, Test Accuracy: 0.89, Time: 35.71 s\n",
      "Epoch 17, Loss: 0.16, Accuracy: 0.95, Test Loss: 0.29, Test Accuracy: 0.89, Time: 35.38 s\n",
      "Epoch 18, Loss: 0.15, Accuracy: 0.96, Test Loss: 0.28, Test Accuracy: 0.89, Time: 35.49 s\n",
      "Epoch 19, Loss: 0.14, Accuracy: 0.96, Test Loss: 0.28, Test Accuracy: 0.89, Time: 35.24 s\n",
      "Epoch 20, Loss: 0.13, Accuracy: 0.96, Test Loss: 0.29, Test Accuracy: 0.89, Time: 35.45 s\n"
     ]
    }
   ],
   "source": [
    "run_cnn_pipeline(dataset, embeddings_size=64, hidden_unit=64, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
